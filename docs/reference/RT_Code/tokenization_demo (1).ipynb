{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding Tokenization\n",
        "\n",
        "This notebook demonstrates how language models break text into tokens:\n",
        "- **What is tokenization?** Breaking text into smaller pieces (tokens)\n",
        "- **Why does it matter?** Different models tokenize differently\n",
        "- **Key insight:** The same sentence can produce different tokens in different models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![raw-tokenized-samples.png](raw-tokenized-samples.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install transformers tiktoken python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Uncomment this to login to Hugging Face\n",
        "\n",
        "from huggingface_hub import login\n",
        "load_dotenv()\n",
        "login(token=os.getenv(\"HF_TOKEN\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Tokenizing with Llama 3.2 1B\n",
        "\n",
        "Let's start by tokenizing a simple sentence using the Llama 3.2 1B tokenizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Llama 3.2 1B tokenizer\n",
        "llama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
        "\n",
        "print(f\"Llama 3.2 1B Tokenizer loaded!\")\n",
        "print(f\"Vocabulary size: {len(llama_tokenizer):,} tokens\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example sentence\n",
        "sentence = \"The hyperparameter-tuning process improved model generalization.\"\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TOKENIZATION WITH LLAMA 3.2 1B\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nOriginal sentence: '{sentence}'\")\n",
        "print(f\"Length: {len(sentence)} characters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize the sentence\n",
        "llama_tokens = llama_tokenizer.encode(sentence, add_special_tokens=False)\n",
        "llama_token_strings = [llama_tokenizer.decode([t]) for t in llama_tokens]\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"TOKENIZATION RESULTS\")\n",
        "print(\"-\"*80)\n",
        "print(f\"\\nNumber of tokens: {len(llama_tokens)}\")\n",
        "print(f\"\\nToken IDs:  {llama_tokens}\")\n",
        "print(f\"\\nToken strings (subwords):\")\n",
        "for i, (token_id, token_str) in enumerate(zip(llama_tokens, llama_token_strings)):\n",
        "    print(f\"  {i}: '{token_str}' (ID: {token_id})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Observations\n",
        "\n",
        "Notice:\n",
        "- Some words are kept whole (e.g., \"The\")\n",
        "- Some words are split into subwords\n",
        "- Spaces are often included with the following word\n",
        "- Each token has a unique ID number\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Tokenizing with Mistral 7B\n",
        "\n",
        "Now let's tokenize the **exact same sentence** using a different model's tokenizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Mistral 7B tokenizer\n",
        "mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
        "\n",
        "print(f\"Mistral 7B Tokenizer loaded!\")\n",
        "print(f\"Vocabulary size: {len(mistral_tokenizer):,} tokens\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize the SAME sentence\n",
        "mistral_tokens = mistral_tokenizer.encode(sentence, add_special_tokens=False)\n",
        "mistral_token_strings = [mistral_tokenizer.decode([t]) for t in mistral_tokens]\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TOKENIZATION WITH MISTRAL 7B\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nOriginal sentence: '{sentence}'\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"TOKENIZATION RESULTS\")\n",
        "print(\"-\"*80)\n",
        "print(f\"\\nNumber of tokens: {len(mistral_tokens)}\")\n",
        "print(f\"\\nToken IDs:  {mistral_tokens}\")\n",
        "print(f\"\\nToken strings (subwords):\")\n",
        "for i, (token_id, token_str) in enumerate(zip(mistral_tokens, mistral_token_strings)):\n",
        "    print(f\"  {i}: '{token_str}' (ID: {token_id})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Comparing Both Tokenizers\n",
        "\n",
        "Let's put them side-by-side to see the differences clearly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"SIDE-BY-SIDE COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nOriginal sentence: '{sentence}'\")\n",
        "print(f\"\\n{'Model':<20} {'# Tokens':<12} {'Vocab Size':<15}\")\n",
        "print(\"-\"*80)\n",
        "print(f\"{'Llama 3.2 1B':<20} {len(llama_tokens):<12} {len(llama_tokenizer):,}\")\n",
        "print(f\"{'Mistral 7B':<20} {len(mistral_tokens):<12} {len(mistral_tokenizer):,}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"TOKEN BREAKDOWN (side-by-side)\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Show tokens side by side\n",
        "max_len = max(len(llama_token_strings), len(mistral_token_strings))\n",
        "print(f\"\\n{'Position':<10} {'Llama 3.2 Token (ID)':<40} {'Mistral 7B Token (ID)':<40}\")\n",
        "print(\"-\"*100)\n",
        "\n",
        "for i in range(max_len):\n",
        "    if i < len(llama_token_strings):\n",
        "        llama_token = f\"'{llama_token_strings[i]}' ({llama_tokens[i]})\"\n",
        "    else:\n",
        "        llama_token = \"-\"\n",
        "    \n",
        "    if i < len(mistral_token_strings):\n",
        "        mistral_token = f\"'{mistral_token_strings[i]}' ({mistral_tokens[i]})\"\n",
        "    else:\n",
        "        mistral_token = \"-\"\n",
        "    \n",
        "    print(f\"{i:<10} {llama_token:<40} {mistral_token:<40}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîë Key Takeaways\n",
        "\n",
        "From this comparison, we can see:\n",
        "\n",
        "1. **Different token counts**: The same sentence produces a different number of tokens\n",
        "2. **Different token IDs**: Even when tokens look similar, they have different IDs\n",
        "3. **Different splitting strategies**: Models break words differently\n",
        "4. **Different vocabulary sizes**: Each model has its own vocabulary\n",
        "\n",
        "**Why does this matter?**\n",
        "- You **cannot** use tokens from one model with another model\n",
        "- Each model needs its own tokenizer\n",
        "- Tokenization affects model performance and behavior\n",
        "- More tokens = more computation during inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 4: Special Tokens\n",
        "\n",
        "Tokenizers also include **special tokens** that have specific meanings:\n",
        "- **BOS (Beginning of Sequence)**: Marks the start of text\n",
        "- **EOS (End of Sequence)**: Marks the end of text\n",
        "- **PAD (Padding)**: Used to make sequences the same length\n",
        "- **UNK (Unknown)**: For tokens not in vocabulary\n",
        "\n",
        "Let's see what special tokens each tokenizer uses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"LLAMA 3.2 1B SPECIAL TOKENS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get all special tokens\n",
        "special_tokens = {\n",
        "    \"BOS token\": (llama_tokenizer.bos_token, llama_tokenizer.bos_token_id),\n",
        "    \"EOS token\": (llama_tokenizer.eos_token, llama_tokenizer.eos_token_id),\n",
        "    \"PAD token\": (llama_tokenizer.pad_token, llama_tokenizer.pad_token_id),\n",
        "    \"UNK token\": (llama_tokenizer.unk_token, llama_tokenizer.unk_token_id),\n",
        "}\n",
        "\n",
        "print(f\"\\n{'Token Type':<20} {'Token':<20} {'Token ID':<15}\")\n",
        "print(\"-\"*80)\n",
        "for token_type, (token, token_id) in special_tokens.items():\n",
        "    token_display = f\"'{token}'\" if token is not None else \"None\"\n",
        "    token_id_display = str(token_id) if token_id is not None else \"None\"\n",
        "    print(f\"{token_type:<20} {token_display:<20} {token_id_display:<15}\")\n",
        "\n",
        "# Show all special tokens dict\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"All special tokens:\")\n",
        "print(llama_tokenizer.special_tokens_map)\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"MISTRAL 7B SPECIAL TOKENS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get all special tokens\n",
        "special_tokens = {\n",
        "    \"BOS token\": (mistral_tokenizer.bos_token, mistral_tokenizer.bos_token_id),\n",
        "    \"EOS token\": (mistral_tokenizer.eos_token, mistral_tokenizer.eos_token_id),\n",
        "    \"PAD token\": (mistral_tokenizer.pad_token, mistral_tokenizer.pad_token_id),\n",
        "    \"UNK token\": (mistral_tokenizer.unk_token, mistral_tokenizer.unk_token_id),\n",
        "}\n",
        "\n",
        "print(f\"\\n{'Token Type':<20} {'Token':<20} {'Token ID':<15}\")\n",
        "print(\"-\"*80)\n",
        "for token_type, (token, token_id) in special_tokens.items():\n",
        "    token_display = f\"'{token}'\" if token is not None else \"None\"\n",
        "    token_id_display = str(token_id) if token_id is not None else \"None\"\n",
        "    print(f\"{token_type:<20} {token_display:<20} {token_id_display:<15}\")\n",
        "\n",
        "# Show all special tokens dict\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"All special tokens:\")\n",
        "print(mistral_tokenizer.special_tokens_map)\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparison of Special Tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"SPECIAL TOKENS COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n{'Token Type':<20} {'Llama 3.2 1B':<30} {'Mistral 7B':<30}\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "token_types = [\"BOS token\", \"EOS token\", \"PAD token\", \"UNK token\"]\n",
        "llama_tokens = [\n",
        "    (llama_tokenizer.bos_token, llama_tokenizer.bos_token_id),\n",
        "    (llama_tokenizer.eos_token, llama_tokenizer.eos_token_id),\n",
        "    (llama_tokenizer.pad_token, llama_tokenizer.pad_token_id),\n",
        "    (llama_tokenizer.unk_token, llama_tokenizer.unk_token_id),\n",
        "]\n",
        "mistral_tokens = [\n",
        "    (mistral_tokenizer.bos_token, mistral_tokenizer.bos_token_id),\n",
        "    (mistral_tokenizer.eos_token, mistral_tokenizer.eos_token_id),\n",
        "    (mistral_tokenizer.pad_token, mistral_tokenizer.pad_token_id),\n",
        "    (mistral_tokenizer.unk_token, mistral_tokenizer.unk_token_id),\n",
        "]\n",
        "\n",
        "for token_type, (llama_tok, llama_id), (mistral_tok, mistral_id) in zip(token_types, llama_tokens, mistral_tokens):\n",
        "    llama_display = f\"'{llama_tok}' (ID: {llama_id})\" if llama_tok is not None else \"None\"\n",
        "    mistral_display = f\"'{mistral_tok}' (ID: {mistral_id})\" if mistral_tok is not None else \"None\"\n",
        "    print(f\"{token_type:<20} {llama_display:<30} {mistral_display:<30}\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Observations on Special Tokens\n",
        "\n",
        "Key points about special tokens:\n",
        "\n",
        "1. **Different representations**: Even though both models have BOS/EOS tokens, they use different strings and IDs\n",
        "2. **Not all tokens are present**: Some models may not have certain special tokens (e.g., PAD or UNK)\n",
        "3. **Critical for training**: These tokens help the model understand:\n",
        "   - Where text begins and ends\n",
        "   - How to handle padding in batches\n",
        "   - What to do with unknown/rare words\n",
        "\n",
        "**Important:** When preparing data for training or inference, you must use the correct special tokens for your specific model!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 5: Base vs Instruct Model Special Tokens\n",
        "\n",
        "Even within the **same model family**, base and instruction-tuned versions can have different special tokens!\n",
        "\n",
        "Let's compare Llama 3.2 1B base vs Llama 3.2 1B Instruct.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Llama 3.2 1B Instruct tokenizer\n",
        "llama_instruct_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "\n",
        "print(f\"Llama 3.2 1B Instruct Tokenizer loaded!\")\n",
        "print(f\"Vocabulary size: {len(llama_instruct_tokenizer):,} tokens\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"LLAMA 3.2 1B BASE - SPECIAL TOKENS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n{'Token Type':<20} {'Token':<30} {'Token ID':<15}\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "base_tokens = [\n",
        "    (\"BOS token\", llama_tokenizer.bos_token, llama_tokenizer.bos_token_id),\n",
        "    (\"EOS token\", llama_tokenizer.eos_token, llama_tokenizer.eos_token_id),\n",
        "    (\"PAD token\", llama_tokenizer.pad_token, llama_tokenizer.pad_token_id),\n",
        "    (\"UNK token\", llama_tokenizer.unk_token, llama_tokenizer.unk_token_id),\n",
        "]\n",
        "\n",
        "for token_type, token, token_id in base_tokens:\n",
        "    token_display = f\"'{token}'\" if token is not None else \"None\"\n",
        "    token_id_display = str(token_id) if token_id is not None else \"None\"\n",
        "    print(f\"{token_type:<20} {token_display:<30} {token_id_display:<15}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"All special tokens:\")\n",
        "print(llama_tokenizer.special_tokens_map)\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"LLAMA 3.2 1B INSTRUCT - SPECIAL TOKENS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n{'Token Type':<20} {'Token':<30} {'Token ID':<15}\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "instruct_tokens = [\n",
        "    (\"BOS token\", llama_instruct_tokenizer.bos_token, llama_instruct_tokenizer.bos_token_id),\n",
        "    (\"EOS token\", llama_instruct_tokenizer.eos_token, llama_instruct_tokenizer.eos_token_id),\n",
        "    (\"PAD token\", llama_instruct_tokenizer.pad_token, llama_instruct_tokenizer.pad_token_id),\n",
        "    (\"UNK token\", llama_instruct_tokenizer.unk_token, llama_instruct_tokenizer.unk_token_id),\n",
        "]\n",
        "\n",
        "for token_type, token, token_id in instruct_tokens:\n",
        "    token_display = f\"'{token}'\" if token is not None else \"None\"\n",
        "    token_id_display = str(token_id) if token_id is not None else \"None\"\n",
        "    print(f\"{token_type:<20} {token_display:<30} {token_id_display:<15}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"All special tokens:\")\n",
        "print(llama_instruct_tokenizer.special_tokens_map)\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Side-by-Side Comparison: Base vs Instruct\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*90)\n",
        "print(\"BASE vs INSTRUCT SPECIAL TOKENS COMPARISON\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "print(f\"\\n{'Token Type':<20} {'Base Model':<35} {'Instruct Model':<35}\")\n",
        "print(\"-\"*90)\n",
        "\n",
        "token_types = [\"BOS token\", \"EOS token\", \"PAD token\", \"UNK token\"]\n",
        "base_tokens = [\n",
        "    (llama_tokenizer.bos_token, llama_tokenizer.bos_token_id),\n",
        "    (llama_tokenizer.eos_token, llama_tokenizer.eos_token_id),\n",
        "    (llama_tokenizer.pad_token, llama_tokenizer.pad_token_id),\n",
        "    (llama_tokenizer.unk_token, llama_tokenizer.unk_token_id),\n",
        "]\n",
        "instruct_tokens = [\n",
        "    (llama_instruct_tokenizer.bos_token, llama_instruct_tokenizer.bos_token_id),\n",
        "    (llama_instruct_tokenizer.eos_token, llama_instruct_tokenizer.eos_token_id),\n",
        "    (llama_instruct_tokenizer.pad_token, llama_instruct_tokenizer.pad_token_id),\n",
        "    (llama_instruct_tokenizer.unk_token, llama_instruct_tokenizer.unk_token_id),\n",
        "]\n",
        "\n",
        "for token_type, (base_tok, base_id), (inst_tok, inst_id) in zip(token_types, base_tokens, instruct_tokens):\n",
        "    base_display = f\"'{base_tok}' (ID: {base_id})\" if base_tok is not None else \"None\"\n",
        "    inst_display = f\"'{inst_tok}' (ID: {inst_id})\" if inst_tok is not None else \"None\"\n",
        "    \n",
        "    # Add marker if they're different\n",
        "    if base_tok != inst_tok or base_id != inst_id:\n",
        "        marker = \" ‚ö†Ô∏è DIFFERENT\"\n",
        "    else:\n",
        "        marker = \" ‚úì Same\"\n",
        "    \n",
        "    print(f\"{token_type:<20} {base_display:<35} {inst_display:<35}{marker}\")\n",
        "\n",
        "print(\"=\"*90)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 6: The Chat Template - How Conversations Are Formatted\n",
        "\n",
        "The instruct model has a **`chat_template`** attribute that the base model doesn't have.\n",
        "\n",
        "This is a **Jinja2 template** that defines exactly how to format conversations (user/assistant/system messages) into the format the model expects.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if chat_template exists\n",
        "print(\"=\"*80)\n",
        "print(\"CHAT TEMPLATE CHECK\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nBase Model has chat_template: {hasattr(llama_tokenizer, 'chat_template') and llama_tokenizer.chat_template is not None}\")\n",
        "print(f\"Instruct Model has chat_template: {hasattr(llama_instruct_tokenizer, 'chat_template') and llama_instruct_tokenizer.chat_template is not None}\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Instruct Model's Chat Template\n",
        "\n",
        "Let's look at the actual chat template used by the Llama 3.2 Instruct model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"INSTRUCT MODEL CHAT TEMPLATE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if llama_instruct_tokenizer.chat_template:\n",
        "    print(\"\\nThe chat template is a Jinja2 template that formats conversations:\")\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(llama_instruct_tokenizer.chat_template)\n",
        "    print(\"-\"*80)\n",
        "else:\n",
        "    print(\"\\nNo chat template found\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the Chat Template\n",
        "\n",
        "Let's see how the chat template automatically formats a conversation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple conversation\n",
        "conversation = [\n",
        "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What about Germany?\"},\n",
        "]\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"EXAMPLE: FORMATTING A CONVERSATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nOriginal conversation (Python list of dicts):\")\n",
        "print(\"-\"*80)\n",
        "for message in conversation:\n",
        "    print(f\"{message['role']}: {message['content']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FORMATTED WITH CHAT TEMPLATE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Apply chat template\n",
        "formatted_text = llama_instruct_tokenizer.apply_chat_template(\n",
        "    conversation, \n",
        "    tokenize=False,  # Get string, not tokens\n",
        "    add_generation_prompt=False\n",
        ")\n",
        "\n",
        "token_ids = llama_instruct_tokenizer.apply_chat_template(\n",
        "    conversation, \n",
        "    tokenize=True,  # Get string, not tokens\n",
        "    add_generation_prompt=False\n",
        ")\n",
        "\n",
        "print(\"\\n\" + formatted_text)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TOKEN IDS\")\n",
        "print(\"=\"*80)\n",
        "print(token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 7: Tiktoken - OpenAI's Fast Tokenizer\n",
        "\n",
        "**Tiktoken** is OpenAI's tokenizer library - it's extremely fast and used for GPT models (GPT-3.5, GPT-4, etc.).\n",
        "\n",
        "Key features:\n",
        "- üöÄ **Very fast** - written in Rust with Python bindings\n",
        "- üéØ **Simple API** - easy to use\n",
        "- üì¶ **Lightweight** - doesn't require downloading large model files\n",
        "- üî¢ **Multiple encodings** - supports different GPT model encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "# Example: Use the encoding for GPT-4\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "\n",
        "# Tokenize text\n",
        "text = \"What is the capital of France?\"\n",
        "\n",
        "tokens = encoding.encode(text)\n",
        "token_strings = [encoding.decode([t]) for t in tokens]\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TIKTOKEN EXAMPLE (GPT-4 encoding)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nText: '{text}'\")\n",
        "print(f\"\\nNumber of tokens: {len(tokens)}\")\n",
        "print(f\"\\nToken IDs: {tokens}\")\n",
        "print(f\"\\nToken strings:\")\n",
        "for i, (token_id, token_str) in enumerate(zip(tokens, token_strings)):\n",
        "    print(f\"  {i}: '{token_str}' (ID: {token_id})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üí° Tiktoken is perfect for:\")\n",
        "print(\"   - Counting tokens for OpenAI API calls\")\n",
        "print(\"   - Estimating costs (OpenAI charges by token)\")\n",
        "print(\"   - Fast tokenization without loading full models\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Common Tiktoken Encodings\n",
        "\n",
        "Different OpenAI models use different encodings:\n",
        "\n",
        "| Encoding | Models | Vocab Size |\n",
        "|----------|--------|------------|\n",
        "| `cl100k_base` | GPT-4, GPT-3.5-turbo, text-embedding-ada-002 | ~100K tokens |\n",
        "| `p50k_base` | Codex models, text-davinci-002, text-davinci-003 | ~50K tokens |\n",
        "| `r50k_base` | GPT-3 models (davinci, curie, babbage, ada) | ~50K tokens |\n",
        "\n",
        "**Quick reference:**\n",
        "```python\n",
        "# For GPT-4 / GPT-3.5-turbo\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# Or get encoding by model name\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "```\n",
        "\n",
        "**Why use Tiktoken?**\n",
        "- No need to load full model weights\n",
        "- ~10-100x faster than other tokenizers\n",
        "- Perfect for token counting and cost estimation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéì Conclusion: Key Takeaways on Tokenization\n",
        "\n",
        "Throughout this notebook, we've explored the fundamental concepts of tokenization in large language models. Here's what you should remember:\n",
        "\n",
        "### 1Ô∏è‚É£ **Different Models = Different Tokenization**\n",
        "- The **same text** produces **different tokens** in different models\n",
        "- Llama 3.2 and Mistral tokenize \"The quick brown fox...\" differently\n",
        "- You **cannot** interchange tokens between models\n",
        "- Each model has its own vocabulary and tokenization strategy\n",
        "\n",
        "### 2Ô∏è‚É£ **Special Tokens Matter**\n",
        "- Every model has special tokens: BOS, EOS, PAD, UNK\n",
        "- These tokens have **different strings and IDs** across models\n",
        "- Special tokens tell the model where text begins, ends, or needs padding\n",
        "- Using the wrong special tokens breaks the model\n",
        "\n",
        "### 3Ô∏è‚É£ **Base vs Instruct: More Than Just Training**\n",
        "- **Base models**: For text completion, no chat structure\n",
        "- **Instruct models**: For conversations, with special formatting\n",
        "- Instruct models add extra tokens for chat formatting: `<|start_header_id|>`, `<|eot_id|>`, etc.\n",
        "- The **chat template** is what makes instruct models conversation-aware\n",
        "\n",
        "### 4Ô∏è‚É£ **Chat Templates Are Essential**\n",
        "- The `chat_template` automatically formats conversations\n",
        "- It's a Jinja2 template that inserts special tokens in the right places\n",
        "- **Without it**: Your conversation structure is lost\n",
        "- **With it**: The model understands user/assistant/system roles\n",
        "\n",
        "### 5Ô∏è‚É£ **Practical Implications**\n",
        "\n",
        "**When building LLM applications:**\n",
        "- ‚úÖ Always use the **correct tokenizer** for your model\n",
        "- ‚úÖ Use the **instruct version** for chat applications\n",
        "- ‚úÖ Use **`apply_chat_template()`** for conversations\n",
        "- ‚úÖ Check the model's **special tokens** before training\n",
        "- ‚ùå Don't mix tokenizers between models\n",
        "- ‚ùå Don't skip the chat template for instruct models\n",
        "\n",
        "### üéØ **Why This Matters**\n",
        "\n",
        "Tokenization is the **first step** in the LLM pipeline:\n",
        "```\n",
        "Text ‚Üí Tokenization ‚Üí Model Processing ‚Üí Output\n",
        "```\n",
        "\n",
        "If you get tokenization wrong:\n",
        "- The model won't understand your input\n",
        "- Special tokens will be misaligned\n",
        "- Chat structure will be lost\n",
        "- Performance will suffer\n",
        "\n",
        "**Get tokenization right, and everything else follows!**\n",
        "\n",
        "---\n",
        "\n",
        "### üìö What's Next?\n",
        "\n",
        "Now that you understand tokenization, you're ready to:\n",
        "- Learn about model architectures\n",
        "- Understand attention mechanisms\n",
        "- Fine-tune models for your specific tasks\n",
        "- Build robust LLM applications\n",
        "\n",
        "**Remember:** Every successful LLM application starts with proper tokenization! üöÄ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
