{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding Padding and Attention Masks\n",
        "\n",
        "This notebook demonstrates how to handle sequences of different lengths using:\n",
        "- **Padding**: Making all sequences the same length\n",
        "- **Attention Masks**: Telling the model which tokens are real vs padding\n",
        "\n",
        "These concepts are essential for batch processing in language models!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![padding.png](padding.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Login to Hugging Face\n",
        "load_dotenv()\n",
        "login(token=os.getenv(\"HF_TOKEN\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer loaded: PreTrainedTokenizerFast\n",
            "Vocabulary size: 128,256 tokens\n",
            "\n",
            "Special tokens:\n",
            "  BOS: <|begin_of_text|> (ID: 128000)\n",
            "  EOS: <|end_of_text|> (ID: 128001)\n",
            "  PAD: None (ID: None)\n"
          ]
        }
      ],
      "source": [
        "# Load Llama 3.2 1B tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
        "\n",
        "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
        "print(f\"Vocabulary size: {len(tokenizer):,} tokens\")\n",
        "print(f\"\\nSpecial tokens:\")\n",
        "print(f\"  BOS: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")\n",
        "print(f\"  EOS: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
        "print(f\"  PAD: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## The Problem - Different Length Sequences\n",
        "\n",
        "Let's create some example sentences of **different lengths** to see why padding is necessary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EXAMPLE SENTENCES (Different Lengths)\n",
            "================================================================================\n",
            "\n",
            "1. \"Hi!\"\n",
            "   Length: 3 characters\n",
            "\n",
            "2. \"How are you?\"\n",
            "   Length: 12 characters\n",
            "\n",
            "3. \"What's the weather like today?\"\n",
            "   Length: 30 characters\n",
            "\n",
            "4. \"I'm working on a machine learning project using transformers.\"\n",
            "   Length: 61 characters\n"
          ]
        }
      ],
      "source": [
        "# Create sentences of varying lengths\n",
        "sentences = [\n",
        "    \"Hi!\",\n",
        "    \"How are you?\",\n",
        "    \"What's the weather like today?\",\n",
        "    \"I'm working on a machine learning project using transformers.\"\n",
        "]\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"EXAMPLE SENTENCES (Different Lengths)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"\\n{i}. \\\"{sentence}\\\"\")\n",
        "    print(f\"   Length: {len(sentence)} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenize Without Padding\n",
        "\n",
        "Let's first tokenize these sentences **without padding** to see what happens:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "TOKENIZATION WITHOUT PADDING\n",
            "================================================================================\n",
            "\n",
            "Sentence 1: \"Hi!\"\n",
            "  Number of tokens: 3\n",
            "  Token IDs: [128000, 13347, 0]\n",
            "\n",
            "Sentence 2: \"How are you?\"\n",
            "  Number of tokens: 5\n",
            "  Token IDs: [128000, 4438, 527, 499, 30]\n",
            "\n",
            "Sentence 3: \"What's the weather like today?\"\n",
            "  Number of tokens: 8\n",
            "  Token IDs: [128000, 3923, 596, 279, 9282, 1093, 3432, 30]\n",
            "\n",
            "Sentence 4: \"I'm working on a machine learning project using transformers.\"\n",
            "  Number of tokens: 12\n",
            "  Token IDs: [128000, 40, 2846, 3318, 389, 264, 5780, 6975, 2447, 1701, 87970, 13]\n",
            "\n",
            "================================================================================\n",
            "❌ PROBLEM: All sequences have different lengths!\n",
            "   Cannot process as a batch - tensors must have the same shape.\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TOKENIZATION WITHOUT PADDING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Tokenize each sentence separately (no padding)\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    tokens = tokenizer.encode(sentence)\n",
        "    print(f\"\\nSentence {i}: \\\"{sentence}\\\"\")\n",
        "    print(f\"  Number of tokens: {len(tokens)}\")\n",
        "    print(f\"  Token IDs: {tokens}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"❌ PROBLEM: All sequences have different lengths!\")\n",
        "print(\"   Cannot process as a batch - tensors must have the same shape.\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## The Solution - Padding\n",
        "\n",
        "**Padding** adds special `<pad>` tokens to shorter sequences to make them all the same length.\n",
        "\n",
        "Let's tokenize the same sentences **with padding**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, set the pad token (Llama models don't have one by default)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(f\"✓ Set pad_token to: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TOKENIZATION WITH PADDING\n",
            "================================================================================\n",
            "\n",
            "✓ All sequences padded to the same length!\n",
            "\n",
            "Batch shape: torch.Size([4, 12])\n",
            "  - 4 sequences (batch size)\n",
            "  - 12 tokens (max sequence length)\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TOKENIZATION WITH PADDING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Tokenize with padding\n",
        "result = tokenizer(\n",
        "    sentences,\n",
        "    padding=True,              # Add padding to make all sequences same length\n",
        "    truncation=False,          # Don't truncate long sequences\n",
        "    return_tensors=\"pt\",       # Return PyTorch tensors\n",
        "    return_attention_mask=True # Return attention mask\n",
        ")\n",
        "\n",
        "input_ids = result[\"input_ids\"]\n",
        "attention_mask = result[\"attention_mask\"]\n",
        "\n",
        "print(f\"\\n✓ All sequences padded to the same length!\")\n",
        "print(f\"\\nBatch shape: {input_ids.shape}\")\n",
        "print(f\"  - {input_ids.shape[0]} sequences (batch size)\")\n",
        "print(f\"  - {input_ids.shape[1]} tokens (max sequence length)\")\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Examining the Padded Sequences\n",
        "\n",
        "Let's look at each sequence in detail to see where padding was added:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "DETAILED VIEW: INPUT IDS (with padding)\n",
            "================================================================================\n",
            "\n",
            "Sequence 1: \"Hi!\"\n",
            "  Real tokens: 3\n",
            "  Padding tokens: 9\n",
            "  Input IDs: [128000, 13347, 0, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001]\n",
            "  └─> Padding starts at position 3\n",
            "\n",
            "Sequence 2: \"How are you?\"\n",
            "  Real tokens: 5\n",
            "  Padding tokens: 7\n",
            "  Input IDs: [128000, 4438, 527, 499, 30, 128001, 128001, 128001, 128001, 128001, 128001, 128001]\n",
            "  └─> Padding starts at position 5\n",
            "\n",
            "Sequence 3: \"What's the weather like today?\"\n",
            "  Real tokens: 8\n",
            "  Padding tokens: 4\n",
            "  Input IDs: [128000, 3923, 596, 279, 9282, 1093, 3432, 30, 128001, 128001, 128001, 128001]\n",
            "  └─> Padding starts at position 8\n",
            "\n",
            "Sequence 4: \"I'm working on a machine learning project using transformers.\"\n",
            "  Real tokens: 12\n",
            "  Padding tokens: 0\n",
            "  Input IDs: [128000, 40, 2846, 3318, 389, 264, 5780, 6975, 2447, 1701, 87970, 13]\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DETAILED VIEW: INPUT IDS (with padding)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, (sentence, ids, mask) in enumerate(zip(sentences, input_ids, attention_mask), 1):\n",
        "    ids_list = ids.tolist()\n",
        "    mask_list = mask.tolist()\n",
        "    \n",
        "    # Count real tokens vs padding tokens\n",
        "    num_real_tokens = sum(mask_list)\n",
        "    num_padding_tokens = len(mask_list) - num_real_tokens\n",
        "    \n",
        "    print(f\"\\nSequence {i}: \\\"{sentence}\\\"\")\n",
        "    print(f\"  Real tokens: {num_real_tokens}\")\n",
        "    print(f\"  Padding tokens: {num_padding_tokens}\")\n",
        "    print(f\"  Input IDs: {ids_list}\")\n",
        "    \n",
        "    # Highlight padding tokens\n",
        "    if num_padding_tokens > 0:\n",
        "        print(f\"  └─> Padding starts at position {num_real_tokens}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Attention Masks - Telling the Model What to Ignore\n",
        "\n",
        "The **attention mask** is a binary array that tells the model:\n",
        "- `1` = Real token (pay attention to this)\n",
        "- `0` = Padding token (ignore this)\n",
        "\n",
        "Let's examine the attention masks:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ATTENTION MASKS\n",
            "================================================================================\n",
            "\n",
            "Sequence 1: \"Hi!\"\n",
            "  Attention Mask: [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "  Legend: 1 = Real token, 0 = Padding token\n",
            "  Visual: ███░░░░░░░░░  (█ = real, ░ = padding)\n",
            "\n",
            "Sequence 2: \"How are you?\"\n",
            "  Attention Mask: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "  Legend: 1 = Real token, 0 = Padding token\n",
            "  Visual: █████░░░░░░░  (█ = real, ░ = padding)\n",
            "\n",
            "Sequence 3: \"What's the weather like today?\"\n",
            "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
            "  Legend: 1 = Real token, 0 = Padding token\n",
            "  Visual: ████████░░░░  (█ = real, ░ = padding)\n",
            "\n",
            "Sequence 4: \"I'm working on a machine learning project using transformers.\"\n",
            "  Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  Legend: 1 = Real token, 0 = Padding token\n",
            "  Visual: ████████████  (█ = real, ░ = padding)\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"ATTENTION MASKS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, (sentence, ids, mask) in enumerate(zip(sentences, input_ids, attention_mask), 1):\n",
        "    ids_list = ids.tolist()\n",
        "    mask_list = mask.tolist()\n",
        "    \n",
        "    print(f\"\\nSequence {i}: \\\"{sentence}\\\"\")\n",
        "    print(f\"  Attention Mask: {mask_list}\")\n",
        "    print(f\"  Legend: 1 = Real token, 0 = Padding token\")\n",
        "    \n",
        "    # Visual representation\n",
        "    print(f\"  Visual: \", end=\"\")\n",
        "    for m in mask_list:\n",
        "        print(\"█\" if m == 1 else \"░\", end=\"\")\n",
        "    print(\"  (█ = real, ░ = padding)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Complete Matrices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INPUT IDS:\n",
            "tensor([[128000,  13347,      0, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "         128001, 128001, 128001],\n",
            "        [128000,   4438,    527,    499,     30, 128001, 128001, 128001, 128001,\n",
            "         128001, 128001, 128001],\n",
            "        [128000,   3923,    596,    279,   9282,   1093,   3432,     30, 128001,\n",
            "         128001, 128001, 128001],\n",
            "        [128000,     40,   2846,   3318,    389,    264,   5780,   6975,   2447,\n",
            "           1701,  87970,     13]])\n",
            "\n",
            "ATTENTION MASK:\n",
            "tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
          ]
        }
      ],
      "source": [
        "print(\"INPUT IDS:\")\n",
        "print(input_ids)\n",
        "print(\"\\nATTENTION MASK:\")\n",
        "print(attention_mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
