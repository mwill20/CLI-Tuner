{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LoRA Parameter Counting Analysis\n",
        "\n",
        "## Goal\n",
        "Demonstrate the parameter efficiency of LoRA by:\n",
        "1. Loading a model and inspecting its linear layer dimensions\n",
        "2. Counting total parameters in each module type (q_proj, k_proj, v_proj, etc.)\n",
        "3. Calculating trainable parameters for LoRA at different ranks (r=8, r=16)\n",
        "4. Comparing different targeting strategies (attention-only vs. all linear layers)\n",
        "\n",
        "This analysis shows why LoRA is so memory-efficient: instead of fine-tuning millions or billions of parameters, we train only a small fraction (typically <1%) while maintaining strong performance.\n",
        "\n",
        "## Key Insight\n",
        "For a linear layer with shape `(m, n)`, full fine-tuning requires `m*n` parameters.\n",
        "\n",
        "LoRA with rank `r` requires only `r*(m+n)` parameters - **a massive reduction!**\n",
        "\n",
        "**Model used**: Llama 3.2 1B Instruct (smaller model for demonstration)\n",
        "\n",
        "![lora-diagram](lora-diagram.webp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Installation (uncomment if needed)\n",
        "!pip install -q torch transformers peft accelerate bitsandbytes pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoConfig\n",
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "# Authenticate with Hugging Face (required for gated models like Llama)\n",
        "\n",
        "\n",
        "# uncomment if you are using google colab\n",
        "# from google.colab import userdata\n",
        "#login(token=userdata.get(\"HF_TOKEN\"))\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "login(token=os.getenv(\"HF_TOKEN\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the order for displaying module types (attention first, then MLP)\n",
        "ORDERED_TAGS = [\n",
        "    \"q_proj\",\n",
        "    \"k_proj\",\n",
        "    \"v_proj\",\n",
        "    \"o_proj\",\n",
        "    \"gate_proj\",\n",
        "    \"up_proj\",\n",
        "    \"down_proj\",\n",
        "]\n",
        "\n",
        "# Define the linear layer types we care about (attention + MLP projections)\n",
        "TARGET_TAGS = {\n",
        "    \"q_proj\": \"q_proj\",  # Query projection (attention)\n",
        "    \"k_proj\": \"k_proj\",  # Key projection (attention)\n",
        "    \"v_proj\": \"v_proj\",  # Value projection (attention)\n",
        "    \"o_proj\": \"o_proj\",  # Output projection (attention)\n",
        "    \"gate_proj\": \"gate_proj\",  # Gate projection (MLP)\n",
        "    \"up_proj\": \"up_proj\",  # Up projection (MLP)\n",
        "    \"down_proj\": \"down_proj\",  # Down projection (MLP)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "\n",
            "Loading meta-llama/Llama-3.2-1B-Instruct...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total model parameters: 1.24B\n",
            "\n"
          ]
        }
      ],
      "source": [
        "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "# Determine device (GPU if available, otherwise CPU)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\\n\")\n",
        "\n",
        "# Load model configuration first (lightweight, just for inspection)\n",
        "cfg = AutoConfig.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "\n",
        "# Load the full model\n",
        "print(f\"Loading {MODEL_ID}...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16,  # Use FP16 for memory efficiency\n",
        "    low_cpu_mem_usage=True,  # Reduce RAM usage during loading\n",
        "    device_map=\"auto\",  # Automatically use available GPUs\n",
        "    trust_remote_code=True,  # Allow custom model code if needed\n",
        ")\n",
        "\n",
        "# Print total model size\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total model parameters: {total_params/1e9:.2f}B\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "MODEL ARCHITECTURE\n",
            "======================================================================\n",
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 2048)\n",
            "    (layers): ModuleList(\n",
            "      (0-15): 16 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
            ")\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"MODEL ARCHITECTURE\")\n",
        "print(\"=\" * 70)\n",
        "print(model)\n",
        "print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def layer_tag(name):\n",
        "    \"\"\"Extract the module type from a parameter name.\"\"\"\n",
        "    for k, v in TARGET_TAGS.items():\n",
        "        if name.endswith(k + \".weight\"):\n",
        "            return v\n",
        "    return None\n",
        "\n",
        "\n",
        "def sort_by_ordered_tags(dataframe, column_name=\"tag\"):\n",
        "    \"\"\"Sort a dataframe by the predefined module order.\"\"\"\n",
        "    df_copy = dataframe.copy()\n",
        "    df_copy[\"_sort_order\"] = df_copy[column_name].apply(\n",
        "        lambda x: ORDERED_TAGS.index(x) if x in ORDERED_TAGS else len(ORDERED_TAGS)\n",
        "    )\n",
        "    df_sorted = df_copy.sort_values(\"_sort_order\").drop(columns=[\"_sort_order\"])\n",
        "    return df_sorted\n",
        "\n",
        "\n",
        "def lora_trainables(layer_dims, rank):\n",
        "    \"\"\"\n",
        "    Calculate trainable parameters for LoRA.\n",
        "    \n",
        "    For a linear layer with shape (m, n):\n",
        "    - Full fine-tuning: m * n parameters\n",
        "    - LoRA with rank r: m*r + r*n = r*(m+n) parameters\n",
        "    \n",
        "    Args:\n",
        "        layer_dims: List of (m, n) tuples for each layer\n",
        "        rank: LoRA rank (r)\n",
        "    \n",
        "    Returns:\n",
        "        Total trainable parameters across all layers\n",
        "    \"\"\"\n",
        "    return sum(rank * (m + n) for (m, n) in layer_dims)\n",
        "\n",
        "\n",
        "def fmt_millions(x):\n",
        "    \"\"\"Format large numbers as millions with 2 decimal places.\"\"\"\n",
        "    return f\"{x/1e6:.2f}M\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "PARAMETER COUNT BY MODULE TYPE\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tag</th>\n",
              "      <th>layers</th>\n",
              "      <th>total_params</th>\n",
              "      <th>total_params_in_millions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>q_proj</td>\n",
              "      <td>16</td>\n",
              "      <td>67108864</td>\n",
              "      <td>67.11M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>k_proj</td>\n",
              "      <td>16</td>\n",
              "      <td>16777216</td>\n",
              "      <td>16.78M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>v_proj</td>\n",
              "      <td>16</td>\n",
              "      <td>16777216</td>\n",
              "      <td>16.78M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>o_proj</td>\n",
              "      <td>16</td>\n",
              "      <td>67108864</td>\n",
              "      <td>67.11M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gate_proj</td>\n",
              "      <td>16</td>\n",
              "      <td>268435456</td>\n",
              "      <td>268.44M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>up_proj</td>\n",
              "      <td>16</td>\n",
              "      <td>268435456</td>\n",
              "      <td>268.44M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>down_proj</td>\n",
              "      <td>16</td>\n",
              "      <td>268435456</td>\n",
              "      <td>268.44M</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         tag  layers  total_params total_params_in_millions\n",
              "4     q_proj      16      67108864                   67.11M\n",
              "2     k_proj      16      16777216                   16.78M\n",
              "6     v_proj      16      16777216                   16.78M\n",
              "3     o_proj      16      67108864                   67.11M\n",
              "1  gate_proj      16     268435456                  268.44M\n",
              "5    up_proj      16     268435456                  268.44M\n",
              "0  down_proj      16     268435456                  268.44M"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Collect information about all linear layers\n",
        "rows = []\n",
        "for name, param in model.named_parameters():\n",
        "    # Focus on 2D weight matrices (linear layers)\n",
        "    if param.ndim == 2 and name.endswith(\".weight\"):\n",
        "        tag = layer_tag(name)\n",
        "        if tag:\n",
        "            m, n = param.shape  # Weight matrix is [out_features, in_features]\n",
        "            rows.append(\n",
        "                {\n",
        "                    \"name\": name,\n",
        "                    \"tag\": tag,\n",
        "                    \"m\": m,  # Output dimension\n",
        "                    \"n\": n,  # Input dimension\n",
        "                    \"params\": m * n,  # Total parameters in this layer\n",
        "                }\n",
        "            )\n",
        "\n",
        "# Create DataFrame for analysis\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# Aggregate by module type\n",
        "total_by_tag = (\n",
        "    df.groupby(\"tag\")\n",
        "    .agg(\n",
        "        layers=(\"name\", \"count\"),  # Number of layers of this type\n",
        "        total_params=(\"params\", \"sum\"),  # Total parameters across all layers\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Sort by predefined order\n",
        "total_by_tag = sort_by_ordered_tags(total_by_tag, column_name=\"tag\")\n",
        "\n",
        "# Format for readability\n",
        "total_by_tag[\"total_params_in_millions\"] = total_by_tag[\"total_params\"].apply(\n",
        "    fmt_millions\n",
        ")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"PARAMETER COUNT BY MODULE TYPE\")\n",
        "print(\"=\" * 70)\n",
        "display(total_by_tag)\n",
        "print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "MANUAL DIMENSION VERIFICATION\n",
            "======================================================================\n",
            "q_proj total:       67.11M\n",
            "k_proj total:       16.78M\n",
            "v_proj total:       16.78M\n",
            "o_proj total:       67.11M\n",
            "gate_proj total:   268.44M\n",
            "up_proj total:     268.44M\n",
            "down_proj total:   268.44M\n",
            "\n",
            "Total linear layer params: 973.08M\n",
            "(This should match the sum from the table above)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"MANUAL DIMENSION VERIFICATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Attention projections\n",
        "q_proj_num_params = 2048 * 2048 * 16  # [hidden_size, hidden_size] × num_layers\n",
        "k_proj_num_params = 2048 * 512 * 16  # [hidden_size, kv_channels] × num_layers\n",
        "v_proj_num_params = 2048 * 512 * 16  # [hidden_size, kv_channels] × num_layers\n",
        "o_proj_num_params = 2048 * 2048 * 16  # [hidden_size, hidden_size] × num_layers\n",
        "\n",
        "# MLP projections\n",
        "gate_proj_num_params = 2048 * 8192 * 16  # [hidden_size, intermediate_size] × num_layers\n",
        "up_proj_num_params = 2048 * 8192 * 16  # [hidden_size, intermediate_size] × num_layers\n",
        "down_proj_num_params = 8192 * 2048 * 16  # [intermediate_size, hidden_size] × num_layers\n",
        "\n",
        "print(f\"q_proj total:    {q_proj_num_params/1e6:>8.2f}M\")\n",
        "print(f\"k_proj total:    {k_proj_num_params/1e6:>8.2f}M\")\n",
        "print(f\"v_proj total:    {v_proj_num_params/1e6:>8.2f}M\")\n",
        "print(f\"o_proj total:    {o_proj_num_params/1e6:>8.2f}M\")\n",
        "print(f\"gate_proj total: {gate_proj_num_params/1e6:>8.2f}M\")\n",
        "print(f\"up_proj total:   {up_proj_num_params/1e6:>8.2f}M\")\n",
        "print(f\"down_proj total: {down_proj_num_params/1e6:>8.2f}M\")\n",
        "\n",
        "total_linear_params = (\n",
        "    q_proj_num_params\n",
        "    + k_proj_num_params\n",
        "    + v_proj_num_params\n",
        "    + o_proj_num_params\n",
        "    + gate_proj_num_params\n",
        "    + up_proj_num_params\n",
        "    + down_proj_num_params\n",
        ")\n",
        "print(f\"\\nTotal linear layer params: {total_linear_params/1e6:.2f}M\")\n",
        "print(\"(This should match the sum from the table above)\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "LORA EFFICIENCY: PER-MODULE ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "If you target ONLY one module type:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>layers</th>\n",
              "      <th>full_params_millions</th>\n",
              "      <th>LoRA_r8_trainables_millions</th>\n",
              "      <th>LoRA_r8_percent</th>\n",
              "      <th>LoRA_r16_trainables_millions</th>\n",
              "      <th>LoRA_r16_percent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>q_proj</td>\n",
              "      <td>16</td>\n",
              "      <td>67.11M</td>\n",
              "      <td>0.52M</td>\n",
              "      <td>0.78</td>\n",
              "      <td>1.05M</td>\n",
              "      <td>1.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>k_proj</td>\n",
              "      <td>16</td>\n",
              "      <td>16.78M</td>\n",
              "      <td>0.33M</td>\n",
              "      <td>1.95</td>\n",
              "      <td>0.66M</td>\n",
              "      <td>3.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>v_proj</td>\n",
              "      <td>16</td>\n",
              "      <td>16.78M</td>\n",
              "      <td>0.33M</td>\n",
              "      <td>1.95</td>\n",
              "      <td>0.66M</td>\n",
              "      <td>3.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>o_proj</td>\n",
              "      <td>16</td>\n",
              "      <td>67.11M</td>\n",
              "      <td>0.52M</td>\n",
              "      <td>0.78</td>\n",
              "      <td>1.05M</td>\n",
              "      <td>1.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gate_proj</td>\n",
              "      <td>16</td>\n",
              "      <td>268.44M</td>\n",
              "      <td>1.31M</td>\n",
              "      <td>0.49</td>\n",
              "      <td>2.62M</td>\n",
              "      <td>0.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>up_proj</td>\n",
              "      <td>16</td>\n",
              "      <td>268.44M</td>\n",
              "      <td>1.31M</td>\n",
              "      <td>0.49</td>\n",
              "      <td>2.62M</td>\n",
              "      <td>0.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>down_proj</td>\n",
              "      <td>16</td>\n",
              "      <td>268.44M</td>\n",
              "      <td>1.31M</td>\n",
              "      <td>0.49</td>\n",
              "      <td>2.62M</td>\n",
              "      <td>0.98</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      target  layers full_params_millions LoRA_r8_trainables_millions  \\\n",
              "4     q_proj      16               67.11M                       0.52M   \n",
              "2     k_proj      16               16.78M                       0.33M   \n",
              "6     v_proj      16               16.78M                       0.33M   \n",
              "3     o_proj      16               67.11M                       0.52M   \n",
              "1  gate_proj      16              268.44M                       1.31M   \n",
              "5    up_proj      16              268.44M                       1.31M   \n",
              "0  down_proj      16              268.44M                       1.31M   \n",
              "\n",
              "   LoRA_r8_percent LoRA_r16_trainables_millions  LoRA_r16_percent  \n",
              "4             0.78                        1.05M              1.56  \n",
              "2             1.95                        0.66M              3.91  \n",
              "6             1.95                        0.66M              3.91  \n",
              "3             0.78                        1.05M              1.56  \n",
              "1             0.49                        2.62M              0.98  \n",
              "5             0.49                        2.62M              0.98  \n",
              "0             0.49                        2.62M              0.98  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"LORA EFFICIENCY: PER-MODULE ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Calculate LoRA parameters for each module type\n",
        "records = []\n",
        "for tag, group in df.groupby(\"tag\"):\n",
        "    # Get dimensions for all layers of this type\n",
        "    mn_list = list(zip(group[\"m\"], group[\"n\"]))\n",
        "    \n",
        "    # Full fine-tuning parameter count\n",
        "    full_params = group[\"params\"].sum()\n",
        "    \n",
        "    # LoRA trainable parameters for r=8 and r=16\n",
        "    lora_r8 = lora_trainables(mn_list, rank=8)\n",
        "    lora_r16 = lora_trainables(mn_list, rank=16)\n",
        "    \n",
        "    records.append(\n",
        "        {\n",
        "            \"target\": tag,\n",
        "            \"layers\": len(mn_list),\n",
        "            \"full_params\": full_params,\n",
        "            \"LoRA_r8_trainables\": lora_r8,\n",
        "            \"LoRA_r16_trainables\": lora_r16,\n",
        "            \"LoRA_r8_percent\": 100 * lora_r8 / full_params if full_params > 0 else 0,\n",
        "            \"LoRA_r16_percent\": 100 * lora_r16 / full_params if full_params > 0 else 0,\n",
        "        }\n",
        "    )\n",
        "\n",
        "per_tag = pd.DataFrame(records)\n",
        "\n",
        "# Sort by predefined order\n",
        "per_tag = sort_by_ordered_tags(per_tag, column_name=\"target\")\n",
        "\n",
        "# Format for display\n",
        "for col in [\"full_params\", \"LoRA_r8_trainables\", \"LoRA_r16_trainables\"]:\n",
        "    per_tag[col + \"_millions\"] = per_tag[col].apply(fmt_millions)\n",
        "\n",
        "# Round percentages\n",
        "per_tag[\"LoRA_r8_percent\"] = per_tag[\"LoRA_r8_percent\"].round(2)\n",
        "per_tag[\"LoRA_r16_percent\"] = per_tag[\"LoRA_r16_percent\"].round(2)\n",
        "\n",
        "print(\"\\nIf you target ONLY one module type:\")\n",
        "display(\n",
        "    per_tag[\n",
        "        [\n",
        "            \"target\",\n",
        "            \"layers\",\n",
        "            \"full_params_millions\",\n",
        "            \"LoRA_r8_trainables_millions\",\n",
        "            \"LoRA_r8_percent\",\n",
        "            \"LoRA_r16_trainables_millions\",\n",
        "            \"LoRA_r16_percent\",\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "LORA EFFICIENCY: COMMON TARGETING SCENARIOS\n",
            "======================================================================\n",
            "\n",
            "Common targeting strategies:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>scenario</th>\n",
              "      <th>matrices</th>\n",
              "      <th>full_params_millions</th>\n",
              "      <th>LoRA_r8_trainables_millions</th>\n",
              "      <th>LoRA_r8_percent</th>\n",
              "      <th>LoRA_r16_trainables_millions</th>\n",
              "      <th>LoRA_r16_percent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>qv_only (style/format tasks)</td>\n",
              "      <td>32</td>\n",
              "      <td>83.89M</td>\n",
              "      <td>0.85M</td>\n",
              "      <td>1.02</td>\n",
              "      <td>1.70M</td>\n",
              "      <td>2.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>qkvo (instruction following)</td>\n",
              "      <td>64</td>\n",
              "      <td>167.77M</td>\n",
              "      <td>1.70M</td>\n",
              "      <td>1.02</td>\n",
              "      <td>3.41M</td>\n",
              "      <td>2.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>all_linear (domain adaptation)</td>\n",
              "      <td>112</td>\n",
              "      <td>973.08M</td>\n",
              "      <td>5.64M</td>\n",
              "      <td>0.58</td>\n",
              "      <td>11.27M</td>\n",
              "      <td>1.16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         scenario  matrices full_params_millions  \\\n",
              "0    qv_only (style/format tasks)        32               83.89M   \n",
              "1    qkvo (instruction following)        64              167.77M   \n",
              "2  all_linear (domain adaptation)       112              973.08M   \n",
              "\n",
              "  LoRA_r8_trainables_millions  LoRA_r8_percent LoRA_r16_trainables_millions  \\\n",
              "0                       0.85M             1.02                        1.70M   \n",
              "1                       1.70M             1.02                        3.41M   \n",
              "2                       5.64M             0.58                       11.27M   \n",
              "\n",
              "   LoRA_r16_percent  \n",
              "0              2.03  \n",
              "1              2.03  \n",
              "2              1.16  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"LORA EFFICIENCY: COMMON TARGETING SCENARIOS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Define common targeting strategies (in display order)\n",
        "SCENARIOS = [\n",
        "    (\"qv_only (style/format tasks)\", [\"q_proj\", \"v_proj\"]),\n",
        "    (\"qkvo (instruction following)\", [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]),\n",
        "    (\n",
        "        \"all_linear (domain adaptation)\",\n",
        "        [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    ),\n",
        "]\n",
        "\n",
        "\n",
        "def calculate_scenario(tags):\n",
        "    \"\"\"Calculate LoRA parameters for a given set of target modules.\"\"\"\n",
        "    filtered = df[df[\"tag\"].isin(tags)]\n",
        "    mn_list = list(zip(filtered[\"m\"], filtered[\"n\"]))\n",
        "    \n",
        "    full_params = int(filtered[\"params\"].sum())\n",
        "    lora_r8 = int(lora_trainables(mn_list, rank=8))\n",
        "    lora_r16 = int(lora_trainables(mn_list, rank=16))\n",
        "    \n",
        "    return {\n",
        "        \"matrices\": len(mn_list),\n",
        "        \"full_params\": full_params,\n",
        "        \"LoRA_r8_trainables\": lora_r8,\n",
        "        \"LoRA_r16_trainables\": lora_r16,\n",
        "        \"LoRA_r8_percent\": 100 * lora_r8 / full_params if full_params > 0 else 0,\n",
        "        \"LoRA_r16_percent\": 100 * lora_r16 / full_params if full_params > 0 else 0,\n",
        "    }\n",
        "\n",
        "\n",
        "# Build scenario table (preserves insertion order)\n",
        "scenario_rows = []\n",
        "for name, tags in SCENARIOS:\n",
        "    scenario_rows.append({\"scenario\": name, **calculate_scenario(tags)})\n",
        "\n",
        "sc_df = pd.DataFrame(scenario_rows)\n",
        "\n",
        "# Format for display\n",
        "for col in [\"full_params\", \"LoRA_r8_trainables\", \"LoRA_r16_trainables\"]:\n",
        "    sc_df[col + \"_millions\"] = sc_df[col].apply(fmt_millions)\n",
        "\n",
        "sc_df[\"LoRA_r8_percent\"] = sc_df[\"LoRA_r8_percent\"].round(2)\n",
        "sc_df[\"LoRA_r16_percent\"] = sc_df[\"LoRA_r16_percent\"].round(2)\n",
        "\n",
        "print(\"\\nCommon targeting strategies:\")\n",
        "display(\n",
        "    sc_df[\n",
        "        [\n",
        "            \"scenario\",\n",
        "            \"matrices\",\n",
        "            \"full_params_millions\",\n",
        "            \"LoRA_r8_trainables_millions\",\n",
        "            \"LoRA_r8_percent\",\n",
        "            \"LoRA_r16_trainables_millions\",\n",
        "            \"LoRA_r16_percent\",\n",
        "        ]\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Applying LoRA with PEFT\n",
        "\n",
        "Let's apply LoRA to our model and verify the trainable parameter count:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-15): 16 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): lora.Linear(\n",
              "            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (k_proj): lora.Linear(\n",
              "            (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (v_proj): lora.Linear(\n",
              "            (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (o_proj): lora.Linear(\n",
              "            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Dropout(p=0.05, inplace=False)\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mo/Desktop/ReadyTensor/certifications/llm-eng/repos/rt-llm-eng-cert-week2/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Create LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # Rank\n",
        "    lora_alpha=32,  # Scaling factor\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Target attention layers\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Apply LoRA to model\n",
        "peft_model = get_peft_model(model, lora_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 2048)\n",
              "        (layers): ModuleList(\n",
              "          (0-15): 16 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "              (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "              (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "peft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BEFORE LoRA:\n",
            "  Total parameters: 1,239,222,272\n",
            "  All parameters trainable: 1,239,222,272 (100%)\n",
            "\n",
            "AFTER LoRA:\n",
            "trainable params: 3,407,872 || all params: 1,239,222,272 || trainable%: 0.2750\n"
          ]
        }
      ],
      "source": [
        "# Print trainable parameters before and after\n",
        "print(\"BEFORE LoRA:\")\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"  Total parameters: {total:,}\")\n",
        "print(f\"  All parameters trainable: {total:,} (100%)\")\n",
        "print()\n",
        "\n",
        "print(\"AFTER LoRA:\")\n",
        "peft_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 2048)\n",
              "        (layers): ModuleList(\n",
              "          (0-15): 16 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "              (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "              (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "peft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-15): 16 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merged_model = peft_model.merge_and_unload()\n",
        "merged_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
